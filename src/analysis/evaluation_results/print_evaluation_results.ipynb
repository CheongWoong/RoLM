{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import jsonlines # Make sure you have this installed: pip install jsonlines\n",
    "\n",
    "# Define your data dimensions\n",
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\", \"MMLU-Pro-Law-100Q\"]\n",
    "model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"Llama-3.1-70B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "# Note: Ensure these match your file naming conventions (e.g., \"zero-shot-cot\" vs \"Zero-shot CoT\")\n",
    "prompting_strategies_file = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "# Mapping for display names in the plot legend\n",
    "prompting_strategies_display = {\n",
    "    \"zero-shot\": \"Zero-shot\",\n",
    "    \"zero-shot-cot\": \"Zero-shot CoT\",\n",
    "    \"few-shot\": \"Few-shot\",\n",
    "    \"few-shot-cot\": \"Few-shot CoT\"\n",
    "}\n",
    "\n",
    "# --- Set your results directory here ---\n",
    "result_dir = \"../../../results\" \n",
    "# Example: result_dir = \"/Users/youruser/project/results\"\n",
    "# -------------------------------------\n",
    "\n",
    "# Initialize matrices to store loaded data\n",
    "accuracy_matrix = np.full((len(dataset_names), len(model_names), len(prompting_strategies_file)), -1.0) # Using -1 to indicate missing data\n",
    "inconsistency_matrix = np.full((len(dataset_names), len(model_names), len(prompting_strategies_file)), -1.0) # Using -1 to indicate missing data\n",
    "\n",
    "# Data loading loop\n",
    "print(\"Loading data...\")\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file):\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy_raw}_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        # The accuracy and consistency values are often nested, assuming 'mean' as per example\n",
    "                        if \"accuracy\" in example and \"mean\" in example[\"accuracy\"]:\n",
    "                            accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        if \"consistency\" in example and \"mean\" in example[\"consistency\"]:\n",
    "                            # Inconsistency is 1 if consistency < 1, 0 otherwise\n",
    "                            inconsistency_list.append(1.0 * (example[\"consistency\"][\"mean\"] < 1))\n",
    "                    \n",
    "                    # Calculate mean, handle cases where lists might be empty\n",
    "                    # If lists are empty (e.g., file was empty or parsing failed to find data), assign -1\n",
    "                    mean_accuracy = np.mean(accuracy_list) if accuracy_list else -1.0\n",
    "                    mean_inconsistency = np.mean(inconsistency_list) if inconsistency_list else -1.0\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except FileNotFoundError:\n",
    "                # print(f\"File not found: {input_path}\")\n",
    "                pass # Matrix is already initialized with -1\n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing {input_path}: {e}\")\n",
    "                pass # Matrix is already initialized with -1\n",
    "print(\"Data loading complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Multimodal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fontsize_title = 30\n",
    "fontsize_xylabel = 24\n",
    "# fontsize_text = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Model').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Model'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Model'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Model')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Model\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Impact of Multimodal Training on Inconsistency', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Model').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Model'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Model'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Model')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Model\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Impact of Instruction Tuning on Inconsistency', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Model').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Model'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Model'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Model')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Model\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Impact of Distillation on Inconsistency', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                dataset_name = dataset_name if \"MMLU-Pro-Law-100Q\" not in dataset_name else \"Law-100Q\"\n",
    "                dataset_name = dataset_name if \"CommonsenseQA\" not in dataset_name else \"CSQA\"\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Task').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"CSQA\", \"QASC\", \"100TFQA\", \"GSM8K\", \"Law-100Q\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Task'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Task'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Task')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Task\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Average Inconsistency Across Tasks', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Prompting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Prompting Strategy').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Prompting Strategy'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Prompting Strategy'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Prompting Strategy')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Prompting Strategy\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Average Inconsistency Across Prompting Strategies', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - AVERAGE INCONSISTENCY PER TASK (ACROSS ALL MODELS AND STRATEGIES)\n",
    "plot_data_all_models_strategies = []\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names): # Iterate through all models\n",
    "        for k, prompting_strategy_raw in enumerate(prompting_strategies_file): # Iterate through all prompting strategies\n",
    "            incons = inconsistency_matrix[i][j][k]\n",
    "            \n",
    "            # Only include if the value is valid (not -1, which indicates missing data)\n",
    "            if incons != -1.0:\n",
    "                model_name = model_name if \"8B\" not in model_name else \"Llama-3.1-8B-Inst\"\n",
    "                model_name = model_name if \"70B\" not in model_name else \"Llama-3.1-70B-Inst\"\n",
    "                model_name = model_name if \"gpt-4o\" not in model_name else \"GPT-4o\"\n",
    "                plot_data_all_models_strategies.append({\n",
    "                    'Task': dataset_name,\n",
    "                    'Model': model_name, # Keep for intermediate data, though not used in final aggregation\n",
    "                    'Prompting Strategy': prompting_strategies_display[prompting_strategy_raw], # Keep for intermediate data\n",
    "                    'Inconsistency': incons\n",
    "                })\n",
    "\n",
    "df_all_data = pd.DataFrame(plot_data_all_models_strategies)\n",
    "\n",
    "# Now, average across all models and prompting strategies for each task\n",
    "df_aggregated_by_task = df_all_data.groupby('Model').agg(\n",
    "    Mean_Inconsistency=('Inconsistency', 'mean'),\n",
    "    Std_Inconsistency=('Inconsistency', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Handle cases where std might be NaN if only one data point was available for a task (set to 0 for plotting)\n",
    "df_aggregated_by_task['Std_Inconsistency'] = df_aggregated_by_task['Std_Inconsistency'].fillna(0)\n",
    "\n",
    "# Define the desired order of tasks (optional, uses dataset_names order by default)\n",
    "desired_task_order = [\"Llama-3.1-8B-Inst\", \"Llama-3.1-70B-Inst\", \"GPT-4o\"]\n",
    "\n",
    "# Convert 'Task' to a categorical type with the specified order\n",
    "df_aggregated_by_task['Model'] = pd.Categorical(\n",
    "    df_aggregated_by_task['Model'],\n",
    "    categories=desired_task_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the new categorical order\n",
    "df = df_aggregated_by_task.sort_values('Model')\n",
    "\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted figure size for 5 bars\n",
    "\n",
    "# Create the bar plot for average inconsistency per task\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Model\",\n",
    "    y=\"Mean_Inconsistency\",\n",
    "    # yerr=df[\"Std_Inconsistency\"], # Add error bars for standard deviation\n",
    "    palette=\"colorblind\", # Using a different palette\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    capsize=0.1 # Add caps to the error bars\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "# ax.set_title('Average Inconsistency Across Model Sizes', fontsize=fontsize_title)\n",
    "ax.set_xlabel(\"\", fontsize=fontsize_xylabel)\n",
    "ax.set_ylabel(\"Average Inconsistency\", fontsize=fontsize_xylabel)\n",
    "\n",
    "# # Add text labels on top of each bar\n",
    "# for i, bar in enumerate(ax.patches):\n",
    "#     mean_val = df['Mean_Inconsistency'].iloc[i]\n",
    "#     std_val = df['Std_Inconsistency'].iloc[i]\n",
    "    \n",
    "#     # Position for the text, considering error bars\n",
    "#     # Adjust vertical alignment based on positive/negative value\n",
    "#     if mean_val >= 0:\n",
    "#         text_y_pos = bar.get_height() + 0.001 # Above the error bar top\n",
    "#         va_align = 'bottom'\n",
    "#     else:\n",
    "#         text_y_pos = bar.get_height() - 0.001 # Below the error bar bottom\n",
    "#         va_align = 'top'\n",
    "\n",
    "#     ax.text(bar.get_x() + bar.get_width() / 2, text_y_pos,\n",
    "#             # f'{mean_val:.3f} ± {std_val:.3f}', # Format to 3 decimal places with std\n",
    "#             f'{mean_val:.3f}',\n",
    "#             ha='center', va=va_align,\n",
    "#             fontsize=fontsize_text, color='black')\n",
    "\n",
    "# Adjust x-axis and y-axis tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=fontsize_xylabel)\n",
    "ax.tick_params(axis='y', labelsize=fontsize_xylabel*0.7)\n",
    "\n",
    "# Adjust layout to prevent overlapping titles/labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\", \"MMLU-Pro-Law-100Q\"]\n",
    "model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"Llama-3.1-70B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\\\\begin{table*}[t]\\n\"\n",
    "latex_code += \"\\\\centering\\n\"\n",
    "latex_code += \"\\\\begin{tabular}{c|c|cccc}\\n\"\n",
    "latex_code += \"\\\\toprule\\n\"\n",
    "latex_code += \"Task & Model & \" + \" & \".join(prompting_strategies) + \" \\\\\\\\\\n\"\n",
    "latex_code += \"\\\\midrule\\\\midrule\\n\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{7}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"Llama-3.1-8B\" and \"Zero-shot\" in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\" and \"CoT\" not in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            # Base idx setting\n",
    "            base_idx = 1 if task in [\"GSM8K\"] else 0\n",
    "            if model == \"Llama-3.1-8B\":\n",
    "                base_idx = 3 if task in [\"GSM8K\"] else 2\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\":\n",
    "                base_idx = 1\n",
    "\n",
    "            base_acc = round(accuracy_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            base_inc = round(inconsistency_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            # if acc > base_acc:\n",
    "            #     acc_str = \"\\\\blue{\" + acc_str + \"}\"\n",
    "            # elif acc < base_acc:\n",
    "            #     acc_str = \"\\\\red{\" + acc_str + \"}\"\n",
    "            # if inc < base_inc:\n",
    "            #     inc_str = \"\\\\blue{\" + inc_str + \"}\"\n",
    "            # elif inc > base_inc:\n",
    "            #     inc_str = \"\\\\red{\" + inc_str + \"}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Close LaTeX table\n",
    "latex_code = latex_code.rstrip(\"\\\\midrule\\n\")  # Remove last midrule\n",
    "latex_code += \"\\\\\\\\\\n\"\n",
    "latex_code += \"\\\\bottomrule\\n\"\n",
    "latex_code += \"\\\\end{tabular}\\n\"\n",
    "latex_code += \"\\\\caption{A comprehensive overview of average accuracy (left) and setwise inconsistency (right).}\\n\"\n",
    "latex_code += \"\\\\label{tab:accuracy_inconsistency}\\n\"\n",
    "latex_code += \"\\\\end{table*}\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "# with open(\"accuracy_inconsistency_table.txt\", \"w\") as fout:\n",
    "#     fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency (majority voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "# model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_majority_voting_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} + Self-consistency \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "with open(\"accuracy_inconsistency_table_self_consistency.txt\", \"w\") as fout:\n",
    "    fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference-guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "dataset_names = [\"QASC\"]\n",
    "# model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}-reference-guided_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} + Reference-guided \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            if strat_idx > 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "# with open(\"accuracy_inconsistency_table_reference_guided.txt\", \"w\") as fout:\n",
    "#     fout.write(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
