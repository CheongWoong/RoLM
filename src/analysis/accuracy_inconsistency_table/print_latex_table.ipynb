{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import jsonlines\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../../../results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\\\\begin{table*}[t!]\\n\"\n",
    "latex_code += \"\\\\centering\\n\"\n",
    "latex_code += \"\\\\caption{Mean accuracy (left) and setwise inconsistency (right) across different tasks, models and prompting strategies. Blue values indicate performance improvement over zero-shot (or the leftmost strategy if not available), while red values denote performance drop.}\\n\"\n",
    "latex_code += \"\\\\label{tab:accuracy_inconsistency}\\n\"\n",
    "latex_code += \"\\\\begin{tabular}{c|c|cccc}\\n\"\n",
    "latex_code += \"\\\\toprule\\n\"\n",
    "latex_code += \"Task & Model & \" + \" & \".join(prompting_strategies) + \" \\\\\\\\\\n\"\n",
    "latex_code += \"\\\\midrule\\\\midrule\\n\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"Llama-3.1-8B\" and \"Zero-shot\" in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\" and \"CoT\" not in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            # Base idx setting\n",
    "            base_idx = 1 if task in [\"GSM8K\"] else 0\n",
    "            if model == \"Llama-3.1-8B\":\n",
    "                base_idx = 3 if task in [\"GSM8K\"] else 2\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\":\n",
    "                base_idx = 1\n",
    "\n",
    "            base_acc = round(accuracy_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            base_inc = round(inconsistency_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            if acc > base_acc:\n",
    "                acc_str = \"\\\\blue{\" + acc_str + \"}\"\n",
    "            elif acc < base_acc:\n",
    "                acc_str = \"\\\\red{\" + acc_str + \"}\"\n",
    "            if inc < base_inc:\n",
    "                inc_str = \"\\\\blue{\" + inc_str + \"}\"\n",
    "            elif inc > base_inc:\n",
    "                inc_str = \"\\\\red{\" + inc_str + \"}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Close LaTeX table\n",
    "latex_code = latex_code.rstrip(\"\\\\midrule\\n\")  # Remove last midrule\n",
    "latex_code += \"\\\\\\\\\\n\"\n",
    "latex_code += \"\\\\bottomrule\\n\"\n",
    "latex_code += \"\\\\end{tabular}\\n\"\n",
    "latex_code += \"\\\\end{table*}\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "with open(\"accuracy_inconsistency_table.txt\", \"w\") as fout:\n",
    "    fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency (majority voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "# model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_majority_voting_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} + Self-consistency \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "with open(\"accuracy_inconsistency_table_self_consistency.txt\", \"w\") as fout:\n",
    "    fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference-guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "dataset_names = [\"QASC\"]\n",
    "# model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}-reference-guided_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} + Reference-guided \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            if strat_idx > 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "# with open(\"accuracy_inconsistency_table_reference_guided.txt\", \"w\") as fout:\n",
    "#     fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 5.0\n",
    "\n",
    "####################\n",
    "\n",
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "# model_names = [\"Phi-3.5-mini-instruct\", \"Phi-3.5-vision-instruct\", \"Llama-3.1-8B\", \"Llama-3.1-8B-Instruct\", \"DeepSeek-R1-Distill-Llama-8B\", \"gpt-4o-2024-11-20\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_activation_steering_{alpha}_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} + Activation steering \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "with open(\"accuracy_inconsistency_table_activation_steering.txt\", \"w\") as fout:\n",
    "    fout.write(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
