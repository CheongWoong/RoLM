{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import jsonlines\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[t]\n",
      "\\centering\n",
      "\\caption{Mean accuracy (left) and setwise inconsistency (right) across different tasks, models and prompting strategies. Blue values indicate performance improvement over zero-shot (or the leftmost strategy if not available), while red values denote performance drop.}\n",
      "\\label{tab:accuracy_inconsistency}\n",
      "\\begin{tabular}{c|c|cccc}\n",
      "\\toprule\n",
      "Task & Model & Zero-shot & Zero-shot CoT & Few-shot & Few-shot CoT \\\\\n",
      "\\midrule\\midrule\n",
      "\\multirow{6}{*}{CommonsenseQA}\n",
      "      & Llama-3.1-8B-Instruct & 0.75 | 0.10 & \\blue{0.76} | \\red{0.25} & \\red{0.68} | \\red{0.51} & - \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{QASC}\n",
      "      & Llama-3.1-8B-Instruct & 0.82 | 0.09 & 0.82 | \\red{0.19} & \\red{0.61} | \\red{0.84} & \\red{0.68} | \\red{0.74} \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{100TFQA}\n",
      "      & Llama-3.1-8B-Instruct & 0.70 | 0.10 & \\blue{0.72} | \\red{0.26} & 0.70 | \\red{0.24} & \\red{0.68} | \\red{0.48} \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{GSM8K}\n",
      "      & Llama-3.1-8B-Instruct & - & 0.54 | 0.75 & - & \\blue{0.79} | \\blue{0.37} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "result_dir = \"../../../results_shared\"\n",
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\\\\begin{table*}[t]\\n\"\n",
    "latex_code += \"\\\\centering\\n\"\n",
    "latex_code += \"\\\\caption{Mean accuracy (left) and setwise inconsistency (right) across different tasks, models and prompting strategies. Blue values indicate performance improvement over zero-shot (or the leftmost strategy if not available), while red values denote performance drop.}\\n\"\n",
    "latex_code += \"\\\\label{tab:accuracy_inconsistency}\\n\"\n",
    "latex_code += \"\\\\begin{tabular}{c|c|cccc}\\n\"\n",
    "latex_code += \"\\\\toprule\\n\"\n",
    "latex_code += \"Task & Model & \" + \" & \".join(prompting_strategies) + \" \\\\\\\\\\n\"\n",
    "latex_code += \"\\\\midrule\\\\midrule\\n\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"Llama-3.1-8B\" and \"Zero-shot\" in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\" and \"CoT\" not in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            # Base idx setting\n",
    "            base_idx = 1 if task in [\"GSM8K\"] else 0\n",
    "            if model == \"Llama-3.1-8B\":\n",
    "                base_idx = 3 if task in [\"GSM8K\"] else 2\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\":\n",
    "                base_idx = 1\n",
    "\n",
    "            base_acc = round(accuracy_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            base_inc = round(inconsistency_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            if acc > base_acc:\n",
    "                acc_str = \"\\\\blue{\" + acc_str + \"}\"\n",
    "            elif acc < base_acc:\n",
    "                acc_str = \"\\\\red{\" + acc_str + \"}\"\n",
    "            if inc < base_inc:\n",
    "                inc_str = \"\\\\blue{\" + inc_str + \"}\"\n",
    "            elif inc > base_inc:\n",
    "                inc_str = \"\\\\red{\" + inc_str + \"}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Close LaTeX table\n",
    "latex_code = latex_code.rstrip(\"\\\\midrule\\n\")  # Remove last midrule\n",
    "latex_code += \"\\\\\\\\\\n\"\n",
    "latex_code += \"\\\\bottomrule\\n\"\n",
    "latex_code += \"\\\\end{tabular}\\n\"\n",
    "latex_code += \"\\\\end{table*}\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "# with open(\"accuracy_inconsistency_table.txt\", \"w\") as fout:\n",
    "#     fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[t]\n",
      "\\centering\n",
      "\\caption{Mean accuracy (left) and setwise inconsistency (right) across different tasks, models and prompting strategies. Blue values indicate performance improvement over zero-shot (or the leftmost strategy if not available), while red values denote performance drop.}\n",
      "\\label{tab:accuracy_inconsistency}\n",
      "\\begin{tabular}{c|c|cccc}\n",
      "\\toprule\n",
      "Task & Model & Zero-shot & Zero-shot CoT & Few-shot & Few-shot CoT \\\\\n",
      "\\midrule\\midrule\n",
      "\\multirow{6}{*}{CommonsenseQA}\n",
      "      & Llama-3.1-8B-Instruct & - & - & - & - \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{QASC}\n",
      "      & Llama-3.1-8B-Instruct & - & - & - & - \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{100TFQA}\n",
      "      & Llama-3.1-8B-Instruct & - & - & - & - \\\\\n",
      "\\midrule\n",
      "\\multirow{6}{*}{GSM8K}\n",
      "      & Llama-3.1-8B-Instruct & - & - & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "result_dir = \"../../../results\"\n",
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "accuracy_matrix, inconsistency_matrix = np.zeros((len(dataset_names), len(model_names), len(prompting_strategies))), np.zeros((len(dataset_names), len(model_names), len(prompting_strategies)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, model_name in enumerate(model_names):\n",
    "        for k, prompting_strategy in enumerate(prompting_strategies):\n",
    "            # Read score file\n",
    "            input_path = f\"{result_dir}/{dataset_name}/{model_name}/{prompting_strategy}_finetuned_predictions.jsonl\"\n",
    "            try:\n",
    "                with jsonlines.open(input_path) as fin:\n",
    "                    accuracy_list, inconsistency_list = [], []\n",
    "                    for example in fin.iter():\n",
    "                        accuracy_list.append(example[\"accuracy\"][\"mean\"])\n",
    "                        inconsistency_list.append(1.0*(example[\"consistency\"][\"mean\"] < 1))\n",
    "                    mean_accuracy = np.mean(accuracy_list)\n",
    "                    mean_inconsistency = np.mean(inconsistency_list)\n",
    "\n",
    "                    accuracy_matrix[i][j][k] = mean_accuracy\n",
    "                    inconsistency_matrix[i][j][k] = mean_inconsistency\n",
    "            except Exception as e:\n",
    "                # print(k, e)\n",
    "                accuracy_matrix[i][j][k] = -1\n",
    "                inconsistency_matrix[i][j][k] = -1\n",
    "                # exit(0)\n",
    "\n",
    "prompting_strategies = [\"Zero-shot\", \"Zero-shot CoT\", \"Few-shot\", \"Few-shot CoT\"]\n",
    "# Create LaTeX table string\n",
    "latex_code = \"\\\\begin{table*}[t]\\n\"\n",
    "latex_code += \"\\\\centering\\n\"\n",
    "latex_code += \"\\\\caption{Mean accuracy (left) and setwise inconsistency (right) across different tasks, models and prompting strategies. Blue values indicate performance improvement over zero-shot (or the leftmost strategy if not available), while red values denote performance drop.}\\n\"\n",
    "latex_code += \"\\\\label{tab:accuracy_inconsistency}\\n\"\n",
    "latex_code += \"\\\\begin{tabular}{c|c|cccc}\\n\"\n",
    "latex_code += \"\\\\toprule\\n\"\n",
    "latex_code += \"Task & Model & \" + \" & \".join(prompting_strategies) + \" \\\\\\\\\\n\"\n",
    "latex_code += \"\\\\midrule\\\\midrule\\n\"\n",
    "\n",
    "# Fill the table with data\n",
    "for task_idx, task in enumerate(dataset_names):\n",
    "    latex_code += f\"\\\\multirow{{6}}{{*}}{{{task}}}\\n\"\n",
    "    for model_idx, model in enumerate(model_names):\n",
    "        latex_code += \"      \"\n",
    "        latex_code += f\"& {model} \"\n",
    "        for strat_idx in range(len(prompting_strategies)):\n",
    "            acc = round(accuracy_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "            inc = round(inconsistency_matrix[task_idx, model_idx, strat_idx], 2)\n",
    "\n",
    "            # Skip conditions\n",
    "            if acc < 0:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"Llama-3.1-8B\" and \"Zero-shot\" in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\" and \"CoT\" not in prompting_strategies[strat_idx]:\n",
    "                latex_code += f\"& - \"\n",
    "                continue\n",
    "\n",
    "            # Base idx setting\n",
    "            base_idx = 1 if task in [\"GSM8K\"] else 0\n",
    "            if model == \"Llama-3.1-8B\":\n",
    "                base_idx = 3 if task in [\"GSM8K\"] else 2\n",
    "            if model == \"DeepSeek-R1-Distill-Llama-8B\":\n",
    "                base_idx = 1\n",
    "\n",
    "            base_acc = round(accuracy_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            base_inc = round(inconsistency_matrix[task_idx, model_idx, base_idx], 2)\n",
    "            acc_str = f\"{acc:.2f}\"\n",
    "            inc_str = f\"{inc:.2f}\"\n",
    "            if acc > base_acc:\n",
    "                acc_str = \"\\\\blue{\" + acc_str + \"}\"\n",
    "            elif acc < base_acc:\n",
    "                acc_str = \"\\\\red{\" + acc_str + \"}\"\n",
    "            if inc < base_inc:\n",
    "                inc_str = \"\\\\blue{\" + inc_str + \"}\"\n",
    "            elif inc > base_inc:\n",
    "                inc_str = \"\\\\red{\" + inc_str + \"}\"\n",
    "            latex_code += f\"& {acc_str} | {inc_str} \"\n",
    "        latex_code += \"\\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Close LaTeX table\n",
    "latex_code = latex_code.rstrip(\"\\\\midrule\\n\")  # Remove last midrule\n",
    "latex_code += \"\\\\\\\\\\n\"\n",
    "latex_code += \"\\\\bottomrule\\n\"\n",
    "latex_code += \"\\\\end{tabular}\\n\"\n",
    "latex_code += \"\\\\end{table*}\"\n",
    "\n",
    "# Display the generated LaTeX table code\n",
    "print(latex_code)\n",
    "\n",
    "# with open(\"accuracy_inconsistency_table.txt\", \"w\") as fout:\n",
    "#     fout.write(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
