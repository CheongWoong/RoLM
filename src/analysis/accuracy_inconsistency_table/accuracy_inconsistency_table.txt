\begin{table*}[t]
\centering
\begin{tabular}{c|c|cccc}
\toprule
Task & Model & Zero-shot & Zero-shot CoT & Few-shot & Few-shot CoT \\
\midrule\midrule
\multirow{7}{*}{CommonsenseQA}
      & Phi-3.5-mini-instruct & 0.75 | 0.10 & 0.76 | 0.21 & 0.73 | 0.10 & - \\
      & Phi-3.5-vision-instruct & 0.76 | 0.09 & 0.73 | 0.30 & 0.74 | 0.10 & - \\
      & Llama-3.1-8B & - & - & 0.72 | 0.11 & - \\
      & Llama-3.1-8B-Instruct & 0.75 | 0.10 & 0.76 | 0.25 & 0.68 | 0.51 & - \\
      & Llama-3.1-70B-Instruct & 0.82 | 0.04 & 0.84 | 0.12 & 0.80 | 0.26 & - \\
      & DeepSeek-R1-Distill-Llama-8B & - & 0.68 | 0.51 & - & - \\
      & gpt-4o-2024-11-20 & 0.85 | 0.07 & 0.84 | 0.08 & 0.87 | 0.04 & - \\
\midrule
\multirow{7}{*}{QASC}
      & Phi-3.5-mini-instruct & 0.76 | 0.09 & 0.79 | 0.16 & 0.77 | 0.10 & 0.82 | 0.17 \\
      & Phi-3.5-vision-instruct & 0.75 | 0.11 & 0.77 | 0.29 & 0.76 | 0.10 & 0.77 | 0.20 \\
      & Llama-3.1-8B & - & - & 0.78 | 0.10 & 0.69 | 0.32 \\
      & Llama-3.1-8B-Instruct & 0.82 | 0.09 & 0.82 | 0.19 & 0.61 | 0.84 & 0.68 | 0.74 \\
      & Llama-3.1-70B-Instruct & 0.91 | 0.04 & 0.92 | 0.05 & 0.90 | 0.21 & 0.92 | 0.05 \\
      & DeepSeek-R1-Distill-Llama-8B & - & 0.69 | 0.52 & - & 0.71 | 0.52 \\
      & gpt-4o-2024-11-20 & 0.92 | 0.05 & 0.91 | 0.06 & 0.94 | 0.04 & 0.93 | 0.04 \\
\midrule
\multirow{7}{*}{MMLU-Pro-Law-100Q}
      & Phi-3.5-mini-instruct & 0.27 | 0.25 & 0.32 | 0.49 & 0.24 | 0.23 & 0.31 | 0.52 \\
      & Phi-3.5-vision-instruct & 0.23 | 0.30 & 0.24 | 0.58 & 0.25 | 0.19 & 0.24 | 0.71 \\
      & Llama-3.1-8B & - & - & 0.24 | 0.24 & 0.26 | 0.39 \\
      & Llama-3.1-8B-Instruct & 0.29 | 0.29 & 0.30 | 0.65 & 0.17 | 0.64 & 0.20 | 0.53 \\
      & Llama-3.1-70B-Instruct & 0.41 | 0.05 & 0.45 | 0.36 & 0.40 | 0.06 & 0.40 | 0.43 \\
      & DeepSeek-R1-Distill-Llama-8B & - & 0.18 | 0.87 & - & 0.21 | 0.90 \\
      & gpt-4o-2024-11-20 & 0.58 | 0.18 & 0.57 | 0.26 & 0.58 | 0.17 & 0.56 | 0.28 \\
\midrule
\multirow{7}{*}{100TFQA}
      & Phi-3.5-mini-instruct & 0.66 | 0.11 & 0.68 | 0.14 & 0.69 | 0.05 & 0.65 | 0.14 \\
      & Phi-3.5-vision-instruct & 0.61 | 0.12 & 0.67 | 0.34 & 0.63 | 0.20 & 0.66 | 0.18 \\
      & Llama-3.1-8B & - & - & 0.73 | 0.10 & 0.67 | 0.32 \\
      & Llama-3.1-8B-Instruct & 0.70 | 0.10 & 0.72 | 0.26 & 0.70 | 0.24 & 0.68 | 0.48 \\
      & Llama-3.1-70B-Instruct & 0.88 | 0.02 & 0.82 | 0.14 & 0.87 | 0.14 & 0.82 | 0.09 \\
      & DeepSeek-R1-Distill-Llama-8B & - & 0.66 | 0.48 & - & 0.68 | 0.55 \\
      & gpt-4o-2024-11-20 & 0.93 | 0.06 & 0.97 | 0.05 & 0.94 | 0.00 & 0.98 | 0.05 \\
\midrule
\multirow{7}{*}{GSM8K}
      & Phi-3.5-mini-instruct & - & 0.52 | 0.68 & - & 0.84 | 0.19 \\
      & Phi-3.5-vision-instruct & - & 0.54 | 0.71 & - & 0.73 | 0.32 \\
      & Llama-3.1-8B & - & - & - & 0.50 | 0.48 \\
      & Llama-3.1-8B-Instruct & - & 0.54 | 0.75 & - & 0.79 | 0.37 \\
      & Llama-3.1-70B-Instruct & - & 0.88 | 0.21 & - & 0.94 | 0.06 \\
      & DeepSeek-R1-Distill-Llama-8B & - & 0.78 | 0.33 & - & 0.86 | 0.28 \\
      & gpt-4o-2024-11-20 & - & 0.91 | 0.13 & - & 0.95 | 0.05 \\
\bottomrule
\end{tabular}
\caption{A comprehensive overview of average accuracy (left) and setwise inconsistency (right).}
\label{tab:accuracy_inconsistency}
\end{table*}