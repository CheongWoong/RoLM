{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../../../results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with consistency (bin chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CommonsenseQA\", \"QASC\", \"100TFQA\", \"GSM8K\"]\n",
    "model_names = [\"Llama-3.1-8B-Instruct\", \"gpt-4o-2024-11-20\"]\n",
    "prompting_strategies = [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]\n",
    "\n",
    "# Read score file\n",
    "scores = {}\n",
    "for model_name in model_names:\n",
    "    for dataset_name in dataset_names:\n",
    "        for prompting_strategy in prompting_strategies:\n",
    "            # if dataset_name == \"CommonsenseQA\" and \"gpt\" in model_name and prompting_strategy == \"zero-shot\":\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     continue\n",
    "            dd = defaultdict(list)\n",
    "\n",
    "            output_dir = f\"{result_dir}/{dataset_name}/{model_name}\"\n",
    "            predictions_path = os.path.join(output_dir, f\"{prompting_strategy}_predictions.jsonl\")\n",
    "            raw_predictions_path = os.path.join(output_dir, f\"{prompting_strategy}_raw_predictions.jsonl\")\n",
    "            try:\n",
    "                with jsonlines.open(predictions_path) as fin:\n",
    "                    id_predictions_map, id_consistency_map = {}, {}\n",
    "                    for example in fin.iter():\n",
    "                        id_predictions_map[example[\"id\"]] = example[\"predictions\"]\n",
    "                        id_consistency_map[example[\"id\"]] = example[\"consistency\"][\"mean\"]\n",
    "                X, Y, Z = [], [], []\n",
    "                with jsonlines.open(raw_predictions_path) as fin:\n",
    "                    for example in fin.iter():\n",
    "                        confidences = []\n",
    "                        for format_id, top_tokens in example[\"top_tokens\"].items():\n",
    "                            confidence = -1\n",
    "                            for ii, top_tokenss in enumerate(top_tokens[::-1]):\n",
    "                                if top_tokenss[0] == id_predictions_map[example[\"id\"]][format_id]:\n",
    "                                    if \"top_probs\" in example:\n",
    "                                        confidence = example[\"top_probs\"][format_id][-(ii+1)][0]\n",
    "                                    else:\n",
    "                                        confidence = np.exp(example[\"top_logprobs\"][format_id][-(ii+1)][0])\n",
    "                                    confidences.append(confidence)\n",
    "                                    break\n",
    "                        if len(confidences) != 8:\n",
    "                            # print(example[\"top_tokens\"])\n",
    "                            # print(len(confidences))\n",
    "                            pass\n",
    "                            # raise Exception\n",
    "                        else:\n",
    "                            mean_confidence = np.mean(confidences)\n",
    "                            X.append(mean_confidence)\n",
    "                            Y.append(id_consistency_map[example[\"id\"]])\n",
    "                            Z.append(1.0*(id_consistency_map[example[\"id\"]] >= 0.99))\n",
    "\n",
    "                            # print(confidences)\n",
    "                            # print()\n",
    "                            id_predictions_map[example[\"id\"]].pop('majority_voting')\n",
    "                            # print(id_predictions_map[example[\"id\"]])\n",
    "\n",
    "                            for ii in range(8):\n",
    "                                zz = list(id_predictions_map[example[\"id\"]].values()).count(id_predictions_map[example[\"id\"]][str(ii)])\n",
    "                                dd[zz].append(confidences[ii])\n",
    "                print(f\"{dataset_name} / {model_name} / {prompting_strategy}\")\n",
    "                for zz in range(1, 9):\n",
    "                    print(f\"{zz} / {len(dd[zz]):4} / {np.mean(dd[zz]):.3f}\")\n",
    "                print()\n",
    "            except:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
