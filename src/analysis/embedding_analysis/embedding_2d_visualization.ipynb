{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_memmap(filepath):\n",
    "    with open(filepath.replace(\".dat\", \".conf\"), \"r\") as fin_config:\n",
    "        memmap_configs = json.load(fin_config)\n",
    "        return np.memmap(filepath, mode=\"r\", shape=tuple(memmap_configs[\"shape\"]), dtype=memmap_configs[\"dtype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../../../results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"embedding_2d_plot\", exist_ok=True)\n",
    "\n",
    "for dataset_name in [\"100TFQA\", \"CommonsenseQA\", \"QASC\", \"GSM8K\"]:\n",
    "    for model_name in [\"Llama-3.1-8B-Instruct\", \"Phi-3.5-mini-instruct\"]:\n",
    "        for prompting_strategy in [\"zero-shot\", \"zero-shot-cot\", \"few-shot\", \"few-shot-cot\"]:\n",
    "            output_dir = f\"{result_dir}/{dataset_name}/{model_name}\"\n",
    "            layer_wise_path = os.path.join(output_dir, f\"{prompting_strategy}_layer_wise_hidden_states.dat\")\n",
    "            head_wise_path = os.path.join(output_dir, f\"{prompting_strategy}_head_wise_hidden_states.dat\")\n",
    "\n",
    "            try:\n",
    "                layer_wise_hidden_states = read_memmap(layer_wise_path)\n",
    "            except:\n",
    "                continue\n",
    "            num_samples, num_formats, num_layers, hidden_size = layer_wise_hidden_states.shape\n",
    "            fig, axes = plt.subplots(1, 1, figsize=(6, 5))\n",
    "            for layer_idx in range(num_layers):\n",
    "                # if layer_idx not in [0, 7, 15, 23, 31]:\n",
    "                if layer_idx not in [31]:\n",
    "                    continue\n",
    "                # Step 1: Prepare input\n",
    "                X = layer_wise_hidden_states[:,:,layer_idx,:].reshape(-1, hidden_size)\n",
    "                Y = np.tile(np.arange(num_formats), num_samples)\n",
    "\n",
    "                # Step 2: PCA Projection\n",
    "                pca = PCA(n_components=2)\n",
    "                X_pca = pca.fit_transform(X)\n",
    "\n",
    "                # Step 3: Plot PCA\n",
    "                scatter = axes.scatter(X_pca[:, 0], X_pca[:, 1], c=Y, cmap=\"Accent\", alpha=0.7)\n",
    "                axes.set_title(f\"Layer {layer_idx}\")\n",
    "                axes.set_xlabel(\"PC1\")\n",
    "                axes.set_ylabel(\"PC2\")\n",
    "\n",
    "                fig.colorbar(scatter, ticks=range(num_formats))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"embedding_2d_plot/pca_2d_{dataset_name}_{model_name}_{prompting_strategy}.pdf\")\n",
    "            plt.show()\n",
    "            print(f\"{dataset_name} / {model_name} / {prompting_strategy} (above)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
