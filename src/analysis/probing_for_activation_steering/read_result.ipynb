{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../../../results\"\n",
    "input_dir = \"../../../preprocessed_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-3.1-8B-Instruct\"\n",
    "dataset_name = \"CommonsenseQA\"\n",
    "prompting_strategy = \"zero-shot\"\n",
    "\n",
    "output_dir = f\"{result_dir}/{dataset_name}/{model_name}\"\n",
    "\n",
    "predictions_path = os.path.join(output_dir, f\"{prompting_strategy}_activation_steering_5.0_predictions.jsonl\")\n",
    "base_predictions_path = os.path.join(output_dir, f\"{prompting_strategy}_predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(predictions_path) as fin:\n",
    "    consistency_list = []\n",
    "    for example in fin.iter():\n",
    "        set_consistency = int(example[\"consistency\"][\"mean\"] > 0.99)\n",
    "        consistency_list.append(set_consistency)\n",
    "\n",
    "with jsonlines.open(base_predictions_path) as fin:\n",
    "    base_consistency_list = []\n",
    "    for example in fin.iter():\n",
    "        set_consistency = int(example[\"consistency\"][\"mean\"] > 0.99)\n",
    "        base_consistency_list.append(set_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "good, bad, neutral, total = 0, 0, 0, 0\n",
    "original_error = 0\n",
    "\n",
    "for score, base_score in zip(consistency_list, base_consistency_list):\n",
    "    if score > base_score:\n",
    "        good += 1\n",
    "    elif score < base_score:\n",
    "        bad += 1\n",
    "    else:\n",
    "        neutral += 1\n",
    "    if base_score < 0.5:\n",
    "        original_error += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 977 samples\n",
      "====================\n",
      "Inconsistency error (baseline): 96 samples\n",
      "\n",
      "0 => 1 (improvement): 7 samples\n",
      "1 => 0 (side effect): 5 samples\n",
      "Net improvement: 2 samples\n",
      "\n",
      "Unchanged: 965 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {total} samples\")\n",
    "print(\"=\"*20)\n",
    "print(f\"Inconsistency error (baseline): {original_error} samples\")\n",
    "print()\n",
    "print(f\"0 => 1 (improvement): {good} samples\")\n",
    "print(f\"1 => 0 (side effect): {bad} samples\")\n",
    "print(f\"Net improvement: {good - bad} samples\")\n",
    "print()\n",
    "print(f\"Unchanged: {neutral} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
