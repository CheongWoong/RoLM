{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_memmap(filepath):\n",
    "    with open(filepath.replace(\".dat\", \".conf\"), \"r\") as fin_config:\n",
    "        memmap_configs = json.load(fin_config)\n",
    "        return np.memmap(filepath, mode=\"r\", shape=tuple(memmap_configs[\"shape\"]), dtype=memmap_configs[\"dtype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FORMATS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"../../../results\"\n",
    "input_dir = \"../../../preprocessed_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-3.1-8B-Instruct\"\n",
    "dataset_name = \"CommonsenseQA\"\n",
    "prompting_strategy = \"zero-shot\"\n",
    "\n",
    "output_dir = f\"{result_dir}/{dataset_name}/{model_name}\"\n",
    "\n",
    "predictions_path = os.path.join(output_dir, f\"{prompting_strategy}_predictions_validation.jsonl\")\n",
    "try:\n",
    "    with jsonlines.open(predictions_path) as fin:\n",
    "        id_predictions_map = {}\n",
    "        for example in fin.iter():\n",
    "            id_predictions_map[example[\"id\"]] = example[\"predictions\"]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "id_majority_level_map = defaultdict(list)\n",
    "with jsonlines.open(predictions_path) as fin:\n",
    "    for example in fin.iter():\n",
    "        id_predictions_map[example[\"id\"]].pop('majority_voting')\n",
    "        for ii in range(NUM_FORMATS):\n",
    "            zz = list(id_predictions_map[example[\"id\"]].values()).count(id_predictions_map[example[\"id\"]][str(ii)])\n",
    "            # print(zz)\n",
    "            # zz = ((zz>7)*1.0) # binarize\n",
    "            # zz = max(8-zz, zz) # pairing\n",
    "            id_majority_level_map[example[\"id\"]].append(zz)\n",
    "        # print()\n",
    "\n",
    "id_majority_level_map = list(id_majority_level_map.values())\n",
    "id_majority_level_map = np.array(id_majority_level_map)\n",
    "\n",
    "id_majority_level_map = id_majority_level_map.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_wise_path = os.path.join(output_dir, f\"{prompting_strategy}_layer_wise_hidden_states_validation.dat\")\n",
    "head_wise_path = os.path.join(output_dir, f\"{prompting_strategy}_head_wise_hidden_states_validation.dat\")\n",
    "\n",
    "layer_wise_hidden_states = read_memmap(layer_wise_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear probing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       0.57      0.67      0.62         6\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       1.00      1.00      1.00       370\n",
      "\n",
      "    accuracy                           0.98       391\n",
      "   macro avg       0.60      0.61      0.60       391\n",
      "weighted avg       0.98      0.98      0.98       391\n",
      "\n",
      "[[  0   2   0   0   0   0]\n",
      " [  0   0   0   3   0   0]\n",
      " [  0   0   4   0   0   0]\n",
      " [  0   2   0   4   0   0]\n",
      " [  0   0   0   0   6   0]\n",
      " [  0   0   0   0   0 370]]\n",
      "********************\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/cwkang/anaconda3/envs/RoLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data2/cwkang/anaconda3/envs/RoLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data2/cwkang/anaconda3/envs/RoLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "num_samples, num_formats, num_layers, hidden_size = layer_wise_hidden_states.shape\n",
    "for layer_idx in range(num_layers):\n",
    "    if layer_idx not in [31]:\n",
    "        continue\n",
    "\n",
    "    # Step 1: Prepare input\n",
    "    X = layer_wise_hidden_states[:,:,layer_idx,:]\n",
    "    X = X.reshape(-1, hidden_size)\n",
    "    Y = id_majority_level_map\n",
    "\n",
    "    print(\"Linear probing\")\n",
    "    # Step 2: 80:20 split for validation to select top-K components\n",
    "    X_fold1, X_fold2, y_fold1, y_fold2 = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "    layer_accuracy_scores = []\n",
    "    for i in range(2):\n",
    "        # Step 3: Standardization\n",
    "        scaler = StandardScaler()\n",
    "        if i == 0:\n",
    "            X_train = scaler.fit_transform(X_fold1)\n",
    "            X_test = scaler.transform(X_fold2)\n",
    "            y_train, y_test = y_fold1, y_fold2\n",
    "        else:\n",
    "            break\n",
    "            X_train = scaler.fit_transform(X_fold2)\n",
    "            X_test = scaler.transform(X_fold1)\n",
    "            y_train, y_test = y_fold2, y_fold1\n",
    "\n",
    "        # Step 4: Train a linear probing model\n",
    "        clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Step 5: Evaluate the model\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        layer_accuracy_scores.append(accuracy)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    # print(f\"Layer {layer_idx}: {np.mean(layer_accuracy_scores):.4f}\")\n",
    "    print(\"*\"*20)\n",
    "\n",
    "print(\"*\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
